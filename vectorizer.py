import re
import pefile as pe
import hashlib
import numpy as np
from sklearn.feature_extraction import FeatureHasher

class FeatureType(object):
    ''' Base class from which each feature type may inherit '''
    name = ''
    dim = 0

    def __repr__(self):
        return '{}({})'.format(self.name, self.dim)

    def raw_features(self, bytez, pe):
        ''' Generate a JSON-able representation of the file '''
        raise (NotImplementedError)

    def process_raw_features(self, raw_obj):
        ''' Generate a feature vector from the raw features '''
        raise (NotImplementedError)

    def feature_vector(self, bytez, lief_binary):
        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently
        if there are significant speedups to be gained from combining the two functions. '''
        return self.process_raw_features(self.raw_features(bytez, lief_binary))


class SectionInfo(FeatureType):
    ''' Information about section names, sizes and entropy.  Uses hashing trick
    to summarize all this section info into a feature vector.
    '''

    name = 'section'
    dim = 5 + 50 + 50 + 50 + 50 + 50

    def __init__(self):
        super(FeatureType, self).__init__()

    @staticmethod
    def _properties(s):
        return [str(c).split('.')[-1] for c in s.characteristics_lists]

    def raw_features(self, bytez, pe):
        if pe is None:
            return {"entry": "", "sections": []}

        # properties of entry point, or if invalid, the first executable section
        # try:
        # except lief.not_found:
        #     # bad entry point, let's find the first executable section
        #     entry_section = ""
        #     for s in pe.sections:
        #         if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:
        #             entry_section = s.name
        #             break

        entry_section = pe.get_section_by_rva(pe.OPTIONAL_HEADER.AddressOfEntryPoint).name
        raw_obj = {"entry": entry_section, "sections": [{
            'name': s.Name,
            'size': s.SizeOfRawData,
            'entropy': s.get_entropy(),
            'vsize': s.Misc_VirtualSize,
            # 'props': self._properties(s)
        } for s in pe.sections]}

        return raw_obj

    def process_raw_features(self, raw_obj):

        sections = raw_obj['sections']
        general = [
            len(sections),  # total number of sections
            # number of sections with nonzero size
            sum(1 for s in sections if s['size'] == 0),
            # number of sections with an empty name
            sum(1 for s in sections if s['name'] == ""),
            # number of RX
            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),
            # number of W
            sum(1 for s in sections if 'MEM_WRITE' in s['props'])
        ]

        # gross characteristics of each section
        section_sizes = [(s['name'], s['size']) for s in sections]
        section_sizes_hashed = FeatureHasher(50, input_type="pair").transform([section_sizes]).toarray()[0]
        section_entropy = [(s['name'], s['entropy']) for s in sections]
        section_entropy_hashed = FeatureHasher(50, input_type="pair").transform([section_entropy]).toarray()[0]
        section_vsize = [(s['name'], s['vsize']) for s in sections]
        section_vsize_hashed = FeatureHasher(50, input_type="pair").transform([section_vsize]).toarray()[0]
        entry_name_hashed = FeatureHasher(50, input_type="string").transform([raw_obj['entry']]).toarray()[0]
        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]
        characteristics_hashed = FeatureHasher(50, input_type="string").transform([characteristics]).toarray()[0]

        return np.hstack([

            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,

            characteristics_hashed

        ]).astype(np.float32)


class PEFeatureExtractor(object):
    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''

    def __init__(self):
        self.features = [
            # ByteHistogram(),
            # ByteEntropyHistogram(),
            # StringExtractor(),
            # GeneralFileInfo(),
            # HeaderFileInfo(),
            SectionInfo(),
            # ImportsInfo(),
            # ExportsInfo()
        ]

        self.dim = sum([fe.dim for fe in self.features])

    def raw_features(self, bytez):
        # #lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,
        #                RuntimeError)

        try:
            lief_binary = lief.PE.parse(list(bytez))

        except lief_errors as e:
            print("lief error: ", str(e))
            lief_binary = None

        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):
            raise

        features = {"sha256": hashlib.sha256(bytez).hexdigest()}
        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})
        return features

    def process_raw_features(self, raw_obj):
        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]
        return np.hstack(feature_vectors).astype(np.float32)

    def feature_vector(self, bytez):
        return self.process_raw_features(self.raw_features(bytez))

    class DataDirectories(FeatureType):
        ''' Extracts size and virtual address of the first 15 data directories '''

        name = 'datadirectories'
        dim = 15 * 2

        def __init__(self):
            super(FeatureType, self).__init__()
            self._name_order = [
                "EXPORT_TABLE", "IMPORT_TABLE", "RESOURCE_TABLE", "EXCEPTION_TABLE", "CERTIFICATE_TABLE",
                "BASE_RELOCATION_TABLE", "DEBUG", "ARCHITECTURE", "GLOBAL_PTR", "TLS_TABLE", "LOAD_CONFIG_TABLE",
                "BOUND_IMPORT", "IAT", "DELAY_IMPORT_DESCRIPTOR", "CLR_RUNTIME_HEADER"
            ]

        def raw_features(self, bytez, pe):
            output = []
            if pe is None:
                return output

            for data_directory in pe.OPTIONAL_HEADER.DATA_DIRECTORY:
                output.append({
                    "name": str(data_directory.name).replace("IMAGE_DIRECTORY_ENTRY_", ""),
                    "size": data_directory.size,
                    "virtual_address": data_directory.rva
                })

        def process_raw_features(self, raw_obj):
            features = np.zeros(2 * len(self._name_order), dtype=np.float32)
            for i in range(len(self._name_order)):
                if i < len(raw_obj):
                    features[2 * i] = raw_obj[i]["size"]
                    features[2 * i + 1] = raw_obj[i]["virtual_address"]
            return features

    class ImportsInfo(FeatureType):
        ''' Information about imported libraries and functions from the
        import address table.  Note that the total number of imported
        functions is contained in GeneralFileInfo.
        '''

        name = 'imports'
        dim = 1280

        def __init__(self):
            super(FeatureType, self).__init__()

        def raw_features(self, bytez, pe):
            imports = {}
            if pe.DIRECTORY_ENTRY_IMPORT is None:
                return imports

            for lib in pe.DIRECTORY_ENTRY_IMPORT:
                if lib.dll not in imports:
                    lib.name = lib.dll.decoded('utf-8')
                    imports[lib.name] = []
                    # libraries can be duplicated in listing, extend instead of overwrite

                # Clipping assumes there are diminishing returns on the discriminatory power of imported functions
                #  beyond the first 10000 characters, and this will help limit the dataset size
                for entry in lib.imports:
                    if entry.ordinal:
                        imports[lib.name].append("ordinal"+str(entry.ordinal))
                    else:
                        imports[lib.name].append(entry.name[:10000])

            return imports

        def process_raw_features(self, raw_obj):
            # unique libraries
            libraries = list(set([l.lower() for l in raw_obj.keys()]))
            libraries_hashed = FeatureHasher(256, input_type="string").transform([libraries]).toarray()[0]

            # A string like "kernel32.dll:CreateFileMappingA" for each imported function
            imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]
            imports_hashed = FeatureHasher(1024, input_type="string").transform([imports]).toarray()[0]

            # Two separate elements: libraries (alone) and fully-qualified names of imported functions
            return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)

    class ExportsInfo(FeatureType):
        ''' Information about exported functions. Note that the total number of exported
        functions is contained in GeneralFileInfo.
        '''

        name = 'exports'
        dim = 128

        def __init__(self):
            super(FeatureType, self).__init__()

        def raw_features(self, bytez, pe):
            if pe is None:
                return []

            # Clipping assumes there are diminishing returns on the discriminatory power of exports beyond
            #  the first 10000 characters, and this will help limit the dataset size
            clipped_exports = [export[:10000] for export in pe.DIRECTORY_ENTRY_EXPORT.symbols]

            return clipped_exports

        def process_raw_features(self, raw_obj):
            exports_hashed = FeatureHasher(128, input_type="string").transform([raw_obj]).toarray()[0]
            return exports_hashed.astype(np.float32)

    class GeneralFileInfo(FeatureType):
        ''' General information about the file '''

        name = 'general'
        dim = 10

        def __init__(self):
            super(FeatureType, self).__init__()

        def raw_features(self, bytez, pe):
            if pe is None:
                return {
                    'size': len(bytez),
                    'vsize': 0,
                    'has_debug': 0,
                    'exports': 0,
                    'imports': 0,
                    'has_relocations': 0,
                    'has_resources': 0,
                    'has_signature': 0,
                    'has_tls': 0,
                    'symbols': 0
                }

            return {
                'size': len(bytez),
                'vsize': pe.OPTIONAL_HEADER.SizeOfImage,
                'has_debug': int(pe.DIRECTORY_ENTRY_DEBUG is not None),
                'exports': len(pe.DIRECTORY_ENTRY_EXPORT.symbols),
                'imports': len(pe.DIRECTORY_ENTRY_IMPORT),
                'has_relocations': int(pe.has_reloc()),
                'has_resources': int(pe.DIRECTORY_ENTRY_RESOURCE is not None),
                'has_signature': int(pe.NT_HEADERS.Signature is not None),
                'has_tls': int(pe.DIRECTORY_ENTRY_TLS is not None),
                'symbols': int(pe.FILE_HEADER.NumberOfSymbols),
            }

        class HeaderFileInfo(FeatureType):
            ''' Machine, architecure, OS, linker and other information extracted from header '''

            name = 'header'
            dim = 62

            def __init__(self):
                super(FeatureType, self).__init__()

            def raw_features(self, bytez, pe):
                raw_obj = {}
                raw_obj['coff'] = {'timestamp': 0, 'machine': "", 'characteristics': []}
                raw_obj['optional'] = {
                    'subsystem': "",
                    'dll_characteristics': [],
                    'magic': "",
                    'major_image_version': 0,
                    'minor_image_version': 0,
                    'major_linker_version': 0,
                    'minor_linker_version': 0,
                    'major_operating_system_version': 0,
                    'minor_operating_system_version': 0,
                    'major_subsystem_version': 0,
                    'minor_subsystem_version': 0,
                    'sizeof_code': 0,
                    'sizeof_headers': 0,
                    'sizeof_heap_commit': 0
                }
                if pe is None:
                    return raw_obj
                # Replacing header.time_date_stamps with TimeDateStamp
                raw_obj['coff']['timestamp'] = pe.FILE_HEADER.TimeDateStamp
                # Lief: .machine
                raw_obj['coff']['machine'] = str(pe.FILE_HEADER.Machine).split('.')[-1]
                # FILE_HEADER.Characteristics , Keep pe.header.characteristics_list for now
                raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in pe.FILE_HEADER.Characteristics]
                # OPTIONAL_HEADER.Subsystem
                raw_obj['optional']['subsystem'] = str(pe.OPTIONAL_HEADER.Subsystem).split('.')[-1]
                raw_obj['optional']['dll_characteristics'] = [
                    # OPTIONAL_HEADER.DllCharacteristics
                    str(c).split('.')[-1] for c in pe.OPTIONAL_HEADER.DllCharacteristics
                ]
                raw_obj['optional']['magic'] = str(pe.OPTIONAL_HEADER.Magic).split('.')[-1]
                raw_obj['optional']['major_image_version'] = pe.OPTIONAL_HEADER.MajorImageVersion
                raw_obj['optional']['minor_image_version'] = pe.OPTIONAL_HEADER.MinorImageVersion
                raw_obj['optional']['major_linker_version'] = pe.OPTIONAL_HEADER.MajorLinkerVersion
                raw_obj['optional']['minor_linker_version'] = pe.OPTIONAL_HEADER.MinorLinkerVersion
                raw_obj['optional'][
                    'major_operating_system_version'] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
                raw_obj['optional'][
                    'minor_operating_system_version'] = pe.OPTIONAL_HEADER.MinorOperatingSystermVersion
                raw_obj['optional']['major_subsystem_version'] = pe.OPTIONAL_HEADER.MajorSubSystemVersion
                raw_obj['optional']['minor_subsystem_version'] = pe.OPTIONAL_HEADER.MinorSubsystemVersion
                # raw_obj['optional']['sizeof_code'] = pe.optional_header.sizeof_code -> Previous from LIEF
                # raw_obj['optional']['sizeof_headers'] = pe.optional_header.sizeof_headers -> Previous from LIEF,
                # replaced with Raw Data
                raw_obj['optional']['sizeof_code'] = pe.OPTIONAL_HEADER.SizeOfRawData
                raw_obj['optional']['sizeof_heap_commit'] = pe.OPTIONAL_HEADER.SizeOfHeapCommit
                return raw_obj;